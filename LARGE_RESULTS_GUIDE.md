# üìä –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Ä–∞–±–æ—Ç–µ —Å –±–æ–ª—å—à–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏

## üéØ –û–±–∑–æ—Ä

Queue Manager –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–¥–∞–Ω–∏–π (–¥–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ–≥–∞–±–∞–π—Ç). –≠—Ç–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è:
- –û–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö
- –ê–Ω–∞–ª–∏–∑–∞ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
- –ì–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–æ–≤
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

### 1. –†–∞–∑–º–µ—Ä —Ä–µ–µ—Å—Ç—Ä–∞
- –ë–æ–ª—å—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ —Ä–µ–µ—Å—Ç—Ä–∞
- –ö–∞–∂–¥—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ JSON –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
- –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: –¥–æ 10MB

### 2. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- –ë–æ–ª—å—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–º–µ–¥–ª—è—é—Ç –∑–∞–≥—Ä—É–∑–∫—É —Ä–µ–µ—Å—Ç—Ä–∞
- –£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –≤—Ä–µ–º—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏/–¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
- –ë–æ–ª—å—à–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏

### 3. –°–µ—Ç–µ–≤—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
- –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤–µ–±-API –±–æ–ª—å—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–≥—É—Ç –ø—Ä–µ–≤—ã—à–∞—Ç—å –ª–∏–º–∏—Ç—ã HTTP
- –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CLI –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

## üöÄ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö

```python
class LargeDataGeneratorJob(QueueJobBase):
    def execute(self) -> None:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–æ–ª—å—à–æ–π –º–∞—Å—Å–∏–≤ –¥–∞–Ω–Ω—ã—Ö
        data = []
        for i in range(100000):  # 100K –∑–∞–ø–∏—Å–µ–π
            data.append({
                "id": i,
                "value": random.uniform(0, 1000),
                "timestamp": time.time(),
                "category": random.choice(["A", "B", "C"])
            })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.set_result({
            "data": data,
            "metadata": {
                "count": len(data),
                "generated_at": time.time(),
                "size_mb": len(json.dumps(data)) / (1024 * 1024)
            }
        })
```

### –ê–Ω–∞–ª–∏–∑ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö

```python
class DataAnalyzerJob(QueueJobBase):
    def execute(self) -> None:
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç
        analysis_result = {
            "statistics": {
                "total_records": 1000000,
                "mean": 500.5,
                "std_dev": 100.2,
                "percentiles": {
                    "25th": 400,
                    "50th": 500,
                    "75th": 600,
                    "95th": 800
                }
            },
            "patterns": [
                {"name": "trend", "confidence": 0.85},
                {"name": "seasonality", "confidence": 0.92}
            ],
            "recommendations": [
                "Implement data partitioning",
                "Add monitoring for anomalies"
            ],
            "sample_data": self._get_sample_data(1000)  # –¢–æ–ª—å–∫–æ –æ–±—Ä–∞–∑–µ—Ü
        }
        
        self.set_result(analysis_result)
```

## üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### 1. –°–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö

```python
import gzip
import base64

class CompressedDataJob(QueueJobBase):
    def execute(self) -> None:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ
        large_data = self._generate_large_data()
        
        # –°–∂–∏–º–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        json_str = json.dumps(large_data)
        compressed = gzip.compress(json_str.encode())
        encoded = base64.b64encode(compressed).decode()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∂–∞—Ç—É—é –≤–µ—Ä—Å–∏—é
        self.set_result({
            "compressed_data": encoded,
            "original_size": len(json_str),
            "compressed_size": len(compressed),
            "compression_ratio": len(compressed) / len(json_str),
            "metadata": {
                "compression": "gzip",
                "encoding": "base64"
            }
        })
    
    def _decompress_result(self, result):
        """–î–µ–∫–æ–º–ø—Ä–µ—Å—Å–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""
        encoded = result["compressed_data"]
        compressed = base64.b64decode(encoded)
        json_str = gzip.decompress(compressed).decode()
        return json.loads(json_str)
```

### 2. –ü–∞–≥–∏–Ω–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```python
class PaginatedDataJob(QueueJobBase):
    def execute(self) -> None:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —á–∞—Å—Ç—è–º
        all_data = []
        page_size = 10000
        
        for page in range(10):  # 10 —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ 10K –∑–∞–ø–∏—Å–µ–π
            page_data = self._generate_page_data(page, page_size)
            all_data.extend(page_data)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        self.set_result({
            "data": all_data,
            "pagination": {
                "total_pages": 10,
                "page_size": page_size,
                "total_records": len(all_data)
            },
            "metadata": {
                "generated_at": time.time(),
                "size_mb": len(json.dumps(all_data)) / (1024 * 1024)
            }
        })
```

### 3. –í—ã–±–æ—Ä–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

```python
class SampledDataJob(QueueJobBase):
    def execute(self) -> None:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        full_data = self._generate_full_dataset()
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –æ–±—Ä–∞–∑–µ—Ü –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        sample_size = 1000
        sample_data = random.sample(full_data, min(sample_size, len(full_data)))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–∑–µ—Ü + –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.set_result({
            "sample_data": sample_data,
            "full_dataset_stats": {
                "total_records": len(full_data),
                "sample_size": len(sample_data),
                "sampling_ratio": len(sample_data) / len(full_data)
            },
            "metadata": {
                "sampling_method": "random",
                "confidence_level": 0.95
            }
        })
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –±–æ–ª—å—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### 1. –†–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞

```python
def check_result_size(result):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""
    size_bytes = len(json.dumps(result))
    size_mb = size_bytes / (1024 * 1024)
    
    if size_mb > 10:
        print(f"‚ö†Ô∏è –ë–æ–ª—å—à–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {size_mb:.2f} MB")
    elif size_mb > 1:
        print(f"üìä –°—Ä–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {size_mb:.2f} MB")
    else:
        print(f"‚úÖ –ù–µ–±–æ–ª—å—à–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {size_mb:.2f} MB")
    
    return size_mb
```

### 2. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

```python
import time

def measure_result_performance():
    """–ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏."""
    start_time = time.time()
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    status = queue.get_job_status("large-job")
    result = status.get('result')
    
    load_time = time.time() - start_time
    
    if result:
        size_mb = len(json.dumps(result)) / (1024 * 1024)
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {size_mb:.2f} MB")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {load_time:.3f}s")
        print(f"üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {size_mb/load_time:.2f} MB/s")
```

## üõ†Ô∏è –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### 1. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞

```python
# ‚úÖ –•–æ—Ä–æ—à–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
result = {
    "metadata": {
        "generated_at": time.time(),
        "version": "1.0",
        "size_mb": calculated_size
    },
    "summary": {
        "total_items": count,
        "processing_time": duration,
        "status": "completed"
    },
    "data": large_data_array,
    "statistics": {
        "mean": mean_value,
        "std_dev": std_value
    }
}

# ‚ùå –ü–ª–æ—Ö–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
result = large_data_array  # –ù–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
```

### 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

```python
def safe_get_large_result(job_id):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""
    try:
        status = queue.get_job_status(job_id)
        result = status.get('result')
        
        if not result:
            return None
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
        size_mb = len(json.dumps(result)) / (1024 * 1024)
        if size_mb > 50:  # –õ–∏–º–∏—Ç 50MB
            raise ValueError(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {size_mb:.2f} MB")
            
        return result
        
    except MemoryError:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
        return None
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
        return None
```

### 3. –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```python
def cleanup_old_results(registry_path, max_age_days=30):
    """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ —Ä–µ–µ—Å—Ç—Ä–∞."""
    cutoff_time = time.time() - (max_age_days * 24 * 3600)
    
    with open(registry_path, 'r') as f:
        lines = f.readlines()
    
    filtered_lines = []
    for line in lines:
        record = json.loads(line)
        if record.get('updated_at'):
            updated_time = time.mktime(time.strptime(record['updated_at'], '%Y-%m-%dT%H:%M:%S.%f'))
            if updated_time > cutoff_time:
                filtered_lines.append(line)
    
    with open(registry_path, 'w') as f:
        f.writelines(filtered_lines)
```

## üéØ –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä

```python
#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã —Å –±–æ–ª—å—à–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏.
"""

import json
import time
import random
from queuemgr.proc_api import proc_queue_system
from queuemgr.jobs.base import QueueJobBase

class BigDataJob(QueueJobBase):
    def __init__(self, job_id: str, params: dict):
        super().__init__(job_id, params)
        self.target_size_mb = params.get("size_mb", 1.0)
        
    def execute(self) -> None:
        print(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º {self.target_size_mb}MB –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ
        data = []
        target_bytes = int(self.target_size_mb * 1024 * 1024)
        current_bytes = 0
        
        while current_bytes < target_bytes:
            item = {
                "id": len(data),
                "value": random.uniform(0, 1000),
                "timestamp": time.time(),
                "data": ''.join(random.choices('ABCDEFGHIJKLMNOPQRSTUVWXYZ', k=100))
            }
            data.append(item)
            current_bytes += len(json.dumps(item))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "data": data,
            "metadata": {
                "size_mb": len(json.dumps(data)) / (1024 * 1024),
                "item_count": len(data),
                "generated_at": time.time()
            }
        }
        
        self.set_result(result)
        print(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(data)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤, {len(json.dumps(data))/(1024*1024):.2f}MB")
        
    def on_start(self) -> None:
        print(f"–ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
    def on_end(self) -> None:
        print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
    def on_error(self, exc: BaseException) -> None:
        print(f"–û—à–∏–±–∫–∞: {exc}")

def main():
    with proc_queue_system() as queue:
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞–Ω–∏–µ —Å –±–æ–ª—å—à–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
        queue.add_job(BigDataJob, "big-data-1", {"size_mb": 2.0})
        queue.start_job("big-data-1")
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        time.sleep(5)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        status = queue.get_job_status("big-data-1")
        result = status.get('result')
        
        if result:
            size_mb = result['metadata']['size_mb']
            item_count = result['metadata']['item_count']
            print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–ª—É—á–µ–Ω: {size_mb:.2f}MB, {item_count} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        else:
            print("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")

if __name__ == "__main__":
    main()
```

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- `queuemgr/examples/large_result_job.py` - –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Å –±–æ–ª—å—à–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
- `queuemgr/examples/result_job.py` - –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
- CLI –∫–æ–º–∞–Ω–¥—ã: `queuemgr-cli job status <job-id>` - –ü—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å: `queuemgr-web` - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

---

**–ì–æ—Ç–æ–≤–æ!** –¢–µ–ø–µ—Ä—å –≤—ã –∑–Ω–∞–µ—Ç–µ, –∫–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –±–æ–ª—å—à–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤ Queue Manager! üöÄ
